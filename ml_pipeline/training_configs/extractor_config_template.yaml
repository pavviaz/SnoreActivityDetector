general:
  model_name: ""
  description: 

  device: "cuda"
  cuda_params:
    pin_memory: True
    num_workers: 6
    persistent_workers: True

  save_only_best: True
  convert_to_fp16: 

  continue_from:  # with original fc layer

  finetune_from:  # with new fc layer
  # restore_scheduler_state: True

  seed: 1234


# if this key is specified, feature extraction procedure will be perfomed before training
prepare_dataset_conf: 


training:
  model_to_use: ""  # docs ; multiple models

  dataset_conf: ""  # doesn't work if "prepare_dataset_conf" is specified

  epochs: 20
  # batch_size: 256  # load everything and rearrange to this batch

  # add more loss funcs
  optimizer:
    type: "adam"  # adamw, radam, sgd
    warmup: False
    lr: 0.001
    weight_decay: 
    betas: 0.9, 0.999
    eps: !!float 1e-08
    amsgrad: False
  
  # add scheduler

  
models_params:
  M5:
    n_channel: 40
    stride: 16
  
  CB:
    ffn_dim: 64
    lstm_dim: 32

  # write conf for other models